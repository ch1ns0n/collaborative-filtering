{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26c3663c",
   "metadata": {},
   "source": [
    "# **Load & basic preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d48e757",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from typing import List, Tuple, Dict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.sparse import csr_matrix, vstack\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import MinMaxScaler, normalize\n",
    "import hnswlib\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1de12241",
   "metadata": {},
   "outputs": [],
   "source": [
    "EVENTS_FILE = \"events.csv\"\n",
    "ITEM_PROP1_FILE = \"item_properties_part1.csv\"\n",
    "ITEM_PROP2_FILE = \"item_properties_part2.csv\"\n",
    "CATEGORY_TREE_FILE = \"category_tree.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1c4e9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = \"cache_reco\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a434c9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVD_DIM = 64                 # embedding dim for CB after TruncatedSVD\n",
    "ITEM_NN_TOPK = 200           # compute top-k item neighbors in item-based CF\n",
    "CF_NEIGHBORS_TOPK = 50      # when scoring per user, use top 50 similar items\n",
    "HNSW_M = 64\n",
    "HNSW_EF_CONSTRUCTION = 200\n",
    "HNSW_EF_SEARCH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6cbb0b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_INTERACTIONS_ACTIVE_USER = 1  # we keep users with >= this interactions before train-test split\n",
    "TRAIN_TEST_LAST_N = 3             # last N interactions per user go to test\n",
    "ALPHAS = [0.1, 0.3, 0.5, 0.7, 0.9]  # for evaluation\n",
    "K_EVAL = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "67d1553a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pickle(obj, path):\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(obj, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ea47459e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8db0464d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    events = pd.read_csv(EVENTS_FILE)\n",
    "    ip1 = pd.read_csv(ITEM_PROP1_FILE)\n",
    "    ip2 = pd.read_csv(ITEM_PROP2_FILE)\n",
    "    cat = pd.read_csv(CATEGORY_TREE_FILE)\n",
    "    item_props = pd.concat([ip1, ip2], axis=0, ignore_index=True)\n",
    "    return events, item_props, cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "61f9518f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_events(events: pd.DataFrame) -> pd.DataFrame:\n",
    "    # keep valid event types\n",
    "    events = events[events['event'].isin(['view', 'addtocart', 'transaction'])].copy()\n",
    "    # parse timestamp to datetime\n",
    "    if not np.issubdtype(events['timestamp'].dtype, np.datetime64):\n",
    "        events['timestamp'] = pd.to_datetime(events['timestamp'], unit='ms', errors='coerce')\n",
    "    # map event type to weights (implicit feedback)\n",
    "    weight_map = {'view': 1, 'addtocart': 4, 'transaction': 10}\n",
    "    events['weight'] = events['event'].map(weight_map)\n",
    "    # sort by visitor + timestamp globally for stable later slicing\n",
    "    events = events.sort_values(['visitorid', 'timestamp']).reset_index(drop=True)\n",
    "    return events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b33e7dd",
   "metadata": {},
   "source": [
    "# **Train/Test split by last-N per user**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "35fb9a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_by_last_n(events: pd.DataFrame, n_last=3) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    train_rows = []\n",
    "    test_rows = []\n",
    "\n",
    "    for visitor, g in events.groupby('visitorid'):\n",
    "        g = g.sort_values('timestamp')\n",
    "        if len(g) > n_last:\n",
    "            train_rows.append(g.iloc[:-n_last])\n",
    "            test_rows.append(g.iloc[-n_last:])\n",
    "        else:\n",
    "            train_rows.append(g)\n",
    "            # no test rows for low-interaction users\n",
    "    train_df = pd.concat(train_rows).reset_index(drop=True)\n",
    "    test_df = pd.concat(test_rows).reset_index(drop=True) if test_rows else pd.DataFrame(columns=events.columns)\n",
    "    print(f\"Train: {len(train_df):,} rows, Test: {len(test_df):,} rows, Unique users: {events['visitorid'].nunique():,}\")\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1ee3fb",
   "metadata": {},
   "source": [
    "# **Build item features matrix (one-hot categories) then SVD -> dense embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ece12e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_item_category_matrix(item_props: pd.DataFrame, category_tree: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Filter category properties: property == 'categoryid'\n",
    "    cats = item_props[item_props['property'] == 'categoryid'][['itemid', 'value']].copy()\n",
    "    cats.rename(columns={'value': 'categoryid'}, inplace=True)\n",
    "    cats['categoryid'] = cats['categoryid'].astype(int)\n",
    "    # Expand via category_tree parents\n",
    "    ct = category_tree.copy()\n",
    "    ct['parentid'] = ct['parentid'].fillna(0).astype(int)\n",
    "    ct['categoryid'] = ct['categoryid'].astype(int)\n",
    "    parent_map = dict(zip(ct['categoryid'], ct['parentid']))\n",
    "\n",
    "    def get_parents(cat_id):\n",
    "        res = []\n",
    "        seen = set()\n",
    "        while cat_id in parent_map and parent_map[cat_id] != 0 and cat_id not in seen:\n",
    "            seen.add(cat_id)\n",
    "            cat_id = parent_map[cat_id]\n",
    "            res.append(cat_id)\n",
    "        return res\n",
    "\n",
    "    extra_rows = []\n",
    "    for _, r in cats.iterrows():\n",
    "        parents = get_parents(r['categoryid'])\n",
    "        for p in parents:\n",
    "            extra_rows.append({'itemid': r['itemid'], 'categoryid': p})\n",
    "\n",
    "    if extra_rows:\n",
    "        cats = pd.concat([cats, pd.DataFrame(extra_rows)], ignore_index=True).drop_duplicates()\n",
    "\n",
    "    # pivot one-hot\n",
    "    item_features = pd.get_dummies(cats.set_index('itemid')['categoryid']).groupby(level=0).max()\n",
    "    return item_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "013e6908",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cb_embeddings(item_features: pd.DataFrame, svd_dim: int = SVD_DIM) -> Tuple[np.ndarray, dict, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Input: item_features: DataFrame index=itemid, columns=category ids (one-hot)\n",
    "    Output: embeddings (n_items, svd_dim), itemid->idx map, idx_to_itemid array\n",
    "    \"\"\"\n",
    "    itemids = item_features.index.to_list()\n",
    "    itemid_to_idx = {iid: i for i, iid in enumerate(itemids)}\n",
    "    idx_to_itemid = np.array(itemids)\n",
    "\n",
    "    # TruncatedSVD on sparse input to get dense embeddings\n",
    "    mat = csr_matrix(item_features.values)\n",
    "    svd = TruncatedSVD(n_components=min(svd_dim, mat.shape[1]-1 if mat.shape[1]>1 else 1), random_state=42)\n",
    "    emb = svd.fit_transform(mat)  # shape (n_items, svd_dim or less)\n",
    "    # normalize embeddings (important for cosine/HNSW)\n",
    "    emb = normalize(emb)\n",
    "    return emb.astype('float32'), itemid_to_idx, idx_to_itemid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475c1721",
   "metadata": {},
   "source": [
    "# **Build HNSW index for CB embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "debabb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hnsw_index(emb: np.ndarray, ef_construction=HNSW_EF_CONSTRUCTION, M=HNSW_M, ef_search=HNSW_EF_SEARCH):\n",
    "    n_items, dim = emb.shape\n",
    "    idx = hnswlib.Index(space='cosine', dim=dim)\n",
    "    idx.init_index(max_elements=n_items, ef_construction=ef_construction, M=M)\n",
    "    ids = np.arange(n_items)\n",
    "    idx.add_items(emb, ids)\n",
    "    idx.set_ef(ef_search)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34849a3c",
   "metadata": {},
   "source": [
    "# **Item-based CF: build item-user sparse matrix and nearest neighbors (top-k)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5c1263ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_item_user_matrix(train_df: pd.DataFrame) -> Tuple[csr_matrix, np.ndarray, Dict[int,int], Dict[int,int]]:\n",
    "    \"\"\"\n",
    "    Build item_user_sparse (n_items x n_users) for neighbors calculation.\n",
    "    Returns sparse matrix, itemids array, itemid->col_idx, userids->row_idx\n",
    "    \"\"\"\n",
    "    # Map users and items to indices\n",
    "    users = train_df['visitorid'].unique()\n",
    "    items = train_df['itemid'].unique()\n",
    "    user_to_idx = {u: i for i, u in enumerate(users)}\n",
    "    item_to_idx = {it: i for i, it in enumerate(items)}\n",
    "    idx_to_item = np.array(items)\n",
    "    idx_to_user = np.array(users)\n",
    "\n",
    "    # Build COO arrays\n",
    "    rows = train_df['itemid'].map(item_to_idx).to_numpy()\n",
    "    cols = train_df['visitorid'].map(user_to_idx).to_numpy()\n",
    "    data = train_df['weight'].to_numpy(dtype='float32')\n",
    "\n",
    "    # item x user\n",
    "    mat = csr_matrix((data, (rows, cols)), shape=(len(items), len(users)))\n",
    "    return mat, idx_to_item, item_to_idx, user_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ceda6ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_item_neighbors(item_user_sparse: csr_matrix, topk=ITEM_NN_TOPK, cache_path=None):\n",
    "    \"\"\"\n",
    "    Build top-k item neighbors using NearestNeighbors on sparse rows (items).\n",
    "    Returns neighbors indices and distances arrays aligned with item index.\n",
    "    Uses cache if available and matches matrix shape.\n",
    "    \"\"\"\n",
    "    import time\n",
    "\n",
    "    n_items, n_users = item_user_sparse.shape\n",
    "\n",
    "    # === 1ï¸âƒ£ Cek cache lama ===\n",
    "    if cache_path and os.path.exists(cache_path):\n",
    "        try:\n",
    "            print(f\"ðŸŸ¢ Found existing cache: {cache_path}\")\n",
    "            item_neighbors_idx, item_neighbors_sims = load_pickle(cache_path)\n",
    "\n",
    "            # Validasi ukuran sama\n",
    "            if item_neighbors_idx.shape[0] == n_items:\n",
    "                print(f\"âœ… Cache valid. Using cached neighbors ({n_items:,} items, topk={topk}).\")\n",
    "                return item_neighbors_idx, item_neighbors_sims\n",
    "            else:\n",
    "                print(f\"âš ï¸ Cache mismatch: cached={item_neighbors_idx.shape[0]}, current={n_items}. Rebuilding...\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Failed to load cache due to: {e}. Rebuilding...\")\n",
    "\n",
    "    # === 2ï¸âƒ£ Bangun ulang jika cache tidak valid ===\n",
    "    print(\"Building item neighbors (item-based CF) ... ini mungkin memakan waktu.\")\n",
    "    nn = NearestNeighbors(n_neighbors=min(topk, n_items), metric=\"cosine\", algorithm=\"brute\", n_jobs=-1)\n",
    "    nn.fit(item_user_sparse)\n",
    "    distances, indices = nn.kneighbors(item_user_sparse, return_distance=True)\n",
    "    sims = 1.0 - distances\n",
    "\n",
    "    # === 3ï¸âƒ£ Simpan hasil baru ===\n",
    "    if cache_path:\n",
    "        with open(cache_path, \"wb\") as f:\n",
    "            pickle.dump((indices, sims), f)\n",
    "        print(f\"ðŸ’¾ Saved new cache to {cache_path}\")\n",
    "\n",
    "    return indices, sims"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c35877e",
   "metadata": {},
   "source": [
    "# **Recommendation functions (CF, CB, Hybrid)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cb98d3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_item_based_cf_scores_for_user(user_id: int, user_item_matrix_useridx_map: Dict[int,int],\n",
    "                                      user_item_matrix: csr_matrix,\n",
    "                                      item_neighbors_idx: np.ndarray, item_neighbors_sims: np.ndarray,\n",
    "                                      topk_scores=CF_NEIGHBORS_TOPK) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Compute CF item scores for a user by aggregating similarities from items the user interacted with.\n",
    "    user_item_matrix: item x user sparse matrix\n",
    "    item_neighbors_idx/sims: arrays shape (n_items, k)\n",
    "    Returns pd.Series indexed by itemid (string/int).\n",
    "    \"\"\"\n",
    "    # map user -> column index\n",
    "    if user_id not in user_item_matrix_useridx_map:\n",
    "        return pd.Series(dtype=float)\n",
    "\n",
    "    uidx = user_item_matrix_useridx_map[user_id]\n",
    "    # get user's interacted items (nonzero rows in column uidx)\n",
    "    user_col = user_item_matrix[:, uidx].toarray().flatten()  # length n_items\n",
    "    interacted_indices = np.where(user_col > 0)[0]\n",
    "    if len(interacted_indices) == 0:\n",
    "        return pd.Series(dtype=float)\n",
    "\n",
    "    n_items = item_neighbors_idx.shape[0]\n",
    "    agg_scores = np.zeros(n_items, dtype='float32')\n",
    "\n",
    "    # For each interacted item, add neighbor similarities\n",
    "    for it_idx in interacted_indices:\n",
    "        neigh_idxs = item_neighbors_idx[it_idx]\n",
    "        neigh_sims = item_neighbors_sims[it_idx]\n",
    "        # accumulate weighted by user's interaction weight\n",
    "        w = user_col[it_idx]\n",
    "        agg_scores[neigh_idxs] += (neigh_sims * w)\n",
    "\n",
    "    # zero out already-seen items\n",
    "    agg_scores[interacted_indices] = 0.0\n",
    "    # return as Series indexed by item index (we'll map to itemids outside)\n",
    "    return pd.Series(agg_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7caaf055",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cb_scores_for_user_by_last_item(user_id: int, user_item_matrix_useridx_map: Dict[int,int],\n",
    "                                        user_item_matrix: csr_matrix,\n",
    "                                        itemid_to_emb_idx: Dict[int,int],\n",
    "                                        emb_index: hnswlib.Index,\n",
    "                                        emb_data: np.ndarray,\n",
    "                                        idx_to_itemid_emb: np.ndarray,\n",
    "                                        top_n=100) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Uses last item the user interacted with as anchor, query HNSW for similar items and return score series.\n",
    "    \"\"\"\n",
    "    if user_id not in user_item_matrix_useridx_map:\n",
    "        return pd.Series(dtype=float)\n",
    "\n",
    "    uidx = user_item_matrix_useridx_map[user_id]\n",
    "    user_col = user_item_matrix[:, uidx].toarray().flatten()\n",
    "    interacted_indices = np.where(user_col > 0)[0]\n",
    "    if len(interacted_indices) == 0:\n",
    "        return pd.Series(dtype=float)\n",
    "\n",
    "    last_idx = interacted_indices[-1]  # index in item_user matrix\n",
    "    # convert this index (in item_user matrix) to itemid then to emb idx if mapping exists\n",
    "    # Note: We'll provide a mapping outside to link item_user index -> emb index\n",
    "    # Here assume item_user index aligns with idx_to_itemid_emb mapping (we'll create that mapping earlier)\n",
    "    if last_idx not in itemid_to_emb_idx:\n",
    "        # fallback: empty\n",
    "        return pd.Series(dtype=float)\n",
    "\n",
    "    emb_idx = itemid_to_emb_idx[last_idx]\n",
    "    k = min(top_n + 1, emb_data.shape[0])\n",
    "    labels, distances = emb_index.knn_query(emb_data[emb_idx], k=k)\n",
    "    labels = labels[0]\n",
    "    distances = distances[0]\n",
    "    # skip self\n",
    "    if len(labels) > 1:\n",
    "        labels = labels[1:]\n",
    "        distances = distances[1:]\n",
    "    sims = 1.0 - distances  # convert dist->sim\n",
    "    # map emb idx back to item_user idx (via idx_to_itemid_emb -> itemid -> item_user index)\n",
    "    items_emb_ids = idx_to_itemid_emb[labels]\n",
    "    scores = pd.Series(sims, index=items_emb_ids)\n",
    "    return scores  # index are item_user idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f16cfadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_cf_cb_scores(cf_scores: pd.Series, cb_scores: pd.Series, alpha=0.7):\n",
    "    \"\"\"\n",
    "    cf_scores and cb_scores are Series indexed by same item index space (build union).\n",
    "    alpha = weight for CF (0..1), (1-alpha) for CB.\n",
    "    Returns Series of hybrid scores.\n",
    "    \"\"\"\n",
    "    if cf_scores.empty and cb_scores.empty:\n",
    "        return pd.Series(dtype=float)\n",
    "    # align union\n",
    "    union_idx = cf_scores.index.union(cb_scores.index)\n",
    "    cf_aligned = cf_scores.reindex(union_idx).fillna(0.0)\n",
    "    cb_aligned = cb_scores.reindex(union_idx).fillna(0.0)\n",
    "    # normalize each to [0,1]\n",
    "    def minmax(s):\n",
    "        if s.max() - s.min() < 1e-9:\n",
    "            return pd.Series(0.0, index=s.index)\n",
    "        return (s - s.min()) / (s.max() - s.min())\n",
    "    cf_norm = minmax(cf_aligned)\n",
    "    cb_norm = minmax(cb_aligned)\n",
    "    hybrid = alpha * cf_norm + (1 - alpha) * cb_norm\n",
    "    hybrid = hybrid.sort_values(ascending=False)\n",
    "    return hybrid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f00d28",
   "metadata": {},
   "source": [
    "# **Evaluation metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4d31c197",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_at_k(recommended: List[int], true_items: List[int], k=5) -> Tuple[float,float]:\n",
    "    rec = recommended[:k]\n",
    "    if len(rec) == 0:\n",
    "        return 0.0, 0.0\n",
    "    hits = len(set(rec) & set(true_items))\n",
    "    precision = hits / k\n",
    "    recall = hits / len(true_items) if len(true_items) > 0 else 0.0\n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "53567c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndcg_at_k(recommended: List[int], true_items: List[int], k=5) -> float:\n",
    "    dcg = 0.0\n",
    "    for i, r in enumerate(recommended[:k]):\n",
    "        if r in true_items:\n",
    "            dcg += 1.0 / np.log2(i + 2)\n",
    "    ideal = sum(1.0/np.log2(i+2) for i in range(min(len(true_items), k)))\n",
    "    return dcg / ideal if ideal > 0 else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dad41f",
   "metadata": {},
   "source": [
    "# **Full evaluation loop (item-based CF + CB hybrid)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9d47898e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_hybrid_item_cf(\n",
    "    train_df, test_df, item_features,\n",
    "    emb, itemuser_idx_to_emb_idx, idx_to_itemid_emb_arr, emb_index,\n",
    "    item_user_sparse, idx_to_itemid_itemuser,\n",
    "    item_neighbors_idx, item_neighbors_sims,\n",
    "    alpha_values, k=5\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluasi hybrid item-based CF + CB dengan checkpoint caching.\n",
    "    Jika proses terhenti di tengah jalan, bisa dilanjut dari checkpoint terakhir.\n",
    "    \"\"\"\n",
    "    print(\"Evaluating hybrid item-based CF + CB with safety checks + checkpointing...\")\n",
    "\n",
    "    # === Setup awal ===\n",
    "    itemuser_idx_to_itemid = np.array(idx_to_itemid_itemuser)\n",
    "    true_items_per_user = test_df.groupby(\"visitorid\")[\"itemid\"].apply(list).to_dict()\n",
    "    total_users = len(true_items_per_user)\n",
    "    print(f\"Total users in test: {total_users:,}\")\n",
    "\n",
    "    cache_path = os.path.join(CACHE_DIR, \"hybrid_eval_checkpoint.pkl\")\n",
    "    final_path = os.path.join(CACHE_DIR, \"hybrid_eval_final.pkl\")\n",
    "\n",
    "    # === Jika checkpoint ada, lanjut dari sana ===\n",
    "    if os.path.exists(cache_path):\n",
    "        print(f\"ðŸŸ¢ Found checkpoint: {cache_path}, resuming evaluation...\")\n",
    "        with open(cache_path, \"rb\") as f:\n",
    "            checkpoint = pickle.load(f)\n",
    "        results = checkpoint.get(\"results\", [])\n",
    "        start_idx = checkpoint.get(\"index\", 0)\n",
    "        alpha_start = checkpoint.get(\"alpha_index\", 0)\n",
    "    else:\n",
    "        print(\"ðŸ†• Starting fresh evaluation from scratch...\")\n",
    "        results = []\n",
    "        start_idx = 0\n",
    "        alpha_start = 0\n",
    "\n",
    "    test_users = list(true_items_per_user.keys())\n",
    "    total_alphas = len(alpha_values)\n",
    "\n",
    "    # === Loop per alpha ===\n",
    "    for a_i, alpha in enumerate(alpha_values[alpha_start:], start=alpha_start):\n",
    "        print(f\"Evaluating Î±={alpha} ({a_i+1}/{total_alphas})...\")\n",
    "        precisions, recalls, hit_rates, ndcgs = [], [], [], []\n",
    "        all_recommended_items = set()\n",
    "\n",
    "        # Jika sedang resume â†’ skip user yang sudah selesai\n",
    "        if a_i == alpha_start:\n",
    "            user_start = start_idx\n",
    "        else:\n",
    "            user_start = 0\n",
    "\n",
    "        for i, u in enumerate(tqdm(test_users[user_start:], total=total_users - user_start)):\n",
    "            true_items = true_items_per_user[u]\n",
    "            user_train_items = train_df[train_df[\"visitorid\"] == u][\"itemid\"].tolist()\n",
    "            if not user_train_items:\n",
    "                continue\n",
    "\n",
    "            # === (1) CF Scores ===\n",
    "            cf_scores = {}\n",
    "            for itemid in user_train_items:\n",
    "                try:\n",
    "                    item_idx = idx_to_itemid_itemuser.index(itemid)\n",
    "                except ValueError:\n",
    "                    continue\n",
    "                neigh_idxs = item_neighbors_idx[item_idx]\n",
    "                neigh_sims = item_neighbors_sims[item_idx]\n",
    "                for ni, sim in zip(neigh_idxs, neigh_sims):\n",
    "                    if ni < len(idx_to_itemid_itemuser):\n",
    "                        cf_scores[ni] = cf_scores.get(ni, 0) + sim\n",
    "\n",
    "            if not cf_scores:\n",
    "                continue\n",
    "\n",
    "            cf_scores = pd.Series(cf_scores)\n",
    "\n",
    "            # === (2) CB Scores ===\n",
    "            cb_scores = {}\n",
    "            last_item = user_train_items[-1]\n",
    "            emb_idx = None\n",
    "            for k_it, v in itemuser_idx_to_emb_idx.items():\n",
    "                if idx_to_itemid_itemuser[k_it] == last_item:\n",
    "                    emb_idx = v\n",
    "                    break\n",
    "\n",
    "            if emb_idx is not None:\n",
    "                try:\n",
    "                    labels, distances = emb_index.knn_query(emb[emb_idx], k=min(200, len(emb)))\n",
    "                    cb_items = labels[0]\n",
    "                    cb_dists = 1 - distances[0]\n",
    "                    for emb_i, dist in zip(cb_items, cb_dists):\n",
    "                        if emb_i < len(idx_to_itemid_emb_arr):\n",
    "                            itemid = idx_to_itemid_emb_arr[emb_i]\n",
    "                            if itemid in idx_to_itemid_itemuser:\n",
    "                                cf_idx = idx_to_itemid_itemuser.index(itemid)\n",
    "                                cb_scores[cf_idx] = dist\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            cb_scores = pd.Series(cb_scores) if cb_scores else pd.Series(dtype=float)\n",
    "\n",
    "            # === (3) Gabungkan ===\n",
    "            cf_scaled = (cf_scores - cf_scores.min()) / (cf_scores.max() - cf_scores.min() + 1e-9)\n",
    "            cb_scaled = (cb_scores - cb_scores.min()) / (cb_scores.max() - cb_scores.min() + 1e-9)\n",
    "            hybrid = alpha * cf_scaled.add((1 - alpha) * cb_scaled, fill_value=0)\n",
    "            hybrid = hybrid.drop(\n",
    "                [idx_to_itemid_itemuser.index(i) for i in user_train_items if i in idx_to_itemid_itemuser],\n",
    "                errors=\"ignore\",\n",
    "            ).sort_values(ascending=False)\n",
    "\n",
    "            rec_itemuser_idxs = [idx for idx in hybrid.index[:k] if idx < len(itemuser_idx_to_itemid)]\n",
    "            rec_itemids = [itemuser_idx_to_itemid[idx] for idx in rec_itemuser_idxs if idx < len(itemuser_idx_to_itemid)]\n",
    "            if not rec_itemids:\n",
    "                continue\n",
    "\n",
    "            # === (4) Hitung metrik ===\n",
    "            hits = len(set(rec_itemids) & set(true_items))\n",
    "            precisions.append(hits / k)\n",
    "            recalls.append(hits / len(true_items))\n",
    "            hit_rates.append(1 if hits > 0 else 0)\n",
    "            all_recommended_items.update(rec_itemids)\n",
    "\n",
    "            dcg = sum(1 / np.log2(i + 2) for i, rec in enumerate(rec_itemids) if rec in true_items)\n",
    "            ideal_dcg = sum(1 / np.log2(i + 2) for i in range(min(len(true_items), k)))\n",
    "            ndcgs.append(dcg / ideal_dcg if ideal_dcg > 0 else 0)\n",
    "\n",
    "            # === Simpan checkpoint tiap 1000 user ===\n",
    "            if i % 1000 == 0 and i > 0:\n",
    "                with open(cache_path, \"wb\") as f:\n",
    "                    pickle.dump({\n",
    "                        \"results\": results,\n",
    "                        \"index\": user_start + i,\n",
    "                        \"alpha_index\": a_i\n",
    "                    }, f)\n",
    "                print(f\"ðŸ’¾ Checkpoint saved at user {user_start + i}/{total_users} for Î±={alpha}\")\n",
    "\n",
    "        # === Simpan hasil alpha ini ===\n",
    "        results.append({\n",
    "            \"alpha\": alpha,\n",
    "            \"Precision@5\": np.mean(precisions) if precisions else 0,\n",
    "            \"Recall@5\": np.mean(recalls) if recalls else 0,\n",
    "            \"HitRate@5\": np.mean(hit_rates) if hit_rates else 0,\n",
    "            \"Coverage\": len(all_recommended_items) / len(item_features) if len(item_features) > 0 else 0,\n",
    "            \"NDCG@5\": np.mean(ndcgs) if ndcgs else 0\n",
    "        })\n",
    "\n",
    "        # Hapus start_idx saat pindah alpha\n",
    "        start_idx = 0\n",
    "\n",
    "    # === Simpan hasil akhir ===\n",
    "    with open(final_path, \"wb\") as f:\n",
    "        pickle.dump(results, f)\n",
    "    print(f\"âœ… Evaluation complete. Saved to {final_path}\")\n",
    "\n",
    "    # Hapus checkpoint terakhir (sudah selesai)\n",
    "    if os.path.exists(cache_path):\n",
    "        os.remove(cache_path)\n",
    "        print(\"ðŸ§¹ Removed temporary checkpoint file.\")\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d44b65c",
   "metadata": {},
   "source": [
    "# **Orchestrator main()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6e3d985b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"Loading data...\")\n",
    "    events, item_props, category_tree = load_data()\n",
    "    events = preprocess_events(events)\n",
    "\n",
    "    # Filter user aktif\n",
    "    events = events[events['visitorid'].notnull()].copy()\n",
    "    user_counts = events['visitorid'].value_counts()\n",
    "    active_users_list = user_counts[user_counts >= MIN_INTERACTIONS_ACTIVE_USER].index\n",
    "    events = events[events['visitorid'].isin(active_users_list)].copy()\n",
    "    print(f\"After filtering active users: total interactions {len(events):,}, unique users {events['visitorid'].nunique():,}\")\n",
    "\n",
    "    print(\"Train/test split (last N per user)...\")\n",
    "    train_df, test_df = train_test_split_by_last_n(events, n_last=TRAIN_TEST_LAST_N)\n",
    "\n",
    "    # Build item feature matrix (content-based)\n",
    "    print(\"Building item category matrix...\")\n",
    "    item_features = build_item_category_matrix(item_props, category_tree)\n",
    "\n",
    "    # âœ… Sinkronisasi item agar CF dan CB memiliki domain yang sama\n",
    "    train_items = set(train_df['itemid'].unique())\n",
    "    feature_items = set(item_features.index)\n",
    "    common_items = list(train_items & feature_items)\n",
    "\n",
    "    print(f\"Items in train: {len(train_items):,}\")\n",
    "    print(f\"Items in features: {len(feature_items):,}\")\n",
    "    print(f\"Common items after sync: {len(common_items):,}\")\n",
    "\n",
    "    # Filter hanya item yang muncul di keduanya\n",
    "    train_df = train_df[train_df['itemid'].isin(common_items)].copy()\n",
    "    test_df = test_df[test_df['itemid'].isin(common_items)].copy()\n",
    "    item_features = item_features.loc[common_items].copy()\n",
    "\n",
    "    # Bangun embedding CB\n",
    "    print(\"Building CB embeddings (TruncatedSVD) ...\")\n",
    "    emb, itemid_to_emb_idx_map, idx_to_itemid_emb_arr = build_cb_embeddings(item_features, svd_dim=SVD_DIM)\n",
    "\n",
    "    # Bangun index HNSW untuk CB\n",
    "    print(\"Building HNSW index on CB embeddings...\")\n",
    "    emb_index = build_hnsw_index(emb, ef_construction=HNSW_EF_CONSTRUCTION, M=HNSW_M, ef_search=HNSW_EF_SEARCH)\n",
    "\n",
    "    # Bangun matriks itemâ€“user untuk CF (item-based)\n",
    "    print(\"Building item-user sparse matrix...\")\n",
    "    item_user_sparse, idx_to_itemid_itemuser, item_to_idx_itemuser, user_to_idx_itemuser = build_item_user_matrix(train_df)\n",
    "\n",
    "    # âœ… Sinkronisasi lagi: pastikan item_user dan CB sama-sama mengenal item\n",
    "    valid_items_cf = set(idx_to_itemid_itemuser)\n",
    "    valid_items_cb = set(itemid_to_emb_idx_map.keys())\n",
    "    common_final_items = list(valid_items_cf & valid_items_cb)\n",
    "    print(f\"Items in CF: {len(valid_items_cf):,}\")\n",
    "    print(f\"Items in CB: {len(valid_items_cb):,}\")\n",
    "    print(f\"Final common items (CF âˆ© CB): {len(common_final_items):,}\")\n",
    "\n",
    "    # Filter kembali matriks CF agar hanya mencakup item yang ada di kedua sisi\n",
    "    keep_idx = [i for i, iid in enumerate(idx_to_itemid_itemuser) if iid in common_final_items]\n",
    "    item_user_sparse = item_user_sparse[keep_idx, :]\n",
    "    idx_to_itemid_itemuser = [idx_to_itemid_itemuser[i] for i in keep_idx]\n",
    "\n",
    "    # Mapping item_user idx â†” emb idx\n",
    "    itemuser_idx_to_emb_idx = {}\n",
    "    for it_idx, itemid in enumerate(idx_to_itemid_itemuser):\n",
    "        if itemid in itemid_to_emb_idx_map:\n",
    "            itemuser_idx_to_emb_idx[it_idx] = itemid_to_emb_idx_map[itemid]\n",
    "\n",
    "    # Reverse mapping emb idx â†’ item_user idx\n",
    "    emb_idx_to_itemuser_idx = {}\n",
    "    itemid_to_itemuser_idx = {itemid: idx for idx, itemid in enumerate(idx_to_itemid_itemuser)}\n",
    "    for emb_idx, itemid in enumerate(idx_to_itemid_emb_arr):\n",
    "        if itemid in itemid_to_itemuser_idx:\n",
    "            emb_idx_to_itemuser_idx[emb_idx] = itemid_to_itemuser_idx[itemid]\n",
    "\n",
    "    # Bangun tetangga item (item-based CF)\n",
    "    neighbors_cache = os.path.join(CACHE_DIR, \"item_neighbors.pkl\")\n",
    "    print(\"Building item neighbors (item-based CF) ... (may take time)\")\n",
    "    item_neighbors_idx, item_neighbors_sims = build_item_neighbors(item_user_sparse, topk=ITEM_NN_TOPK, cache_path=neighbors_cache)\n",
    "\n",
    "    # âœ… Proteksi tambahan terhadap mismatch indeks\n",
    "    print(f\"item_neighbors_idx shape: {item_neighbors_idx.shape}, item_user_sparse items: {len(idx_to_itemid_itemuser)}\")\n",
    "\n",
    "    # Evaluasi hybrid (CF + CB)\n",
    "    print(\"Starting evaluation of hybrid item-based CF + CB ...\")\n",
    "    results_df = evaluate_hybrid_item_cf(\n",
    "        train_df, test_df, item_features,\n",
    "        emb, itemuser_idx_to_emb_idx, idx_to_itemid_emb_arr, emb_index,\n",
    "        item_user_sparse, idx_to_itemid_itemuser,\n",
    "        item_neighbors_idx, item_neighbors_sims,\n",
    "        alpha_values=ALPHAS, k=K_EVAL\n",
    "    )\n",
    "\n",
    "    print(\"\\n===== Evaluation Results =====\")\n",
    "    print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaebc07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "After filtering active users: total interactions 2,756,101, unique users 1,407,580\n",
      "Train/test split (last N per user)...\n",
      "Train: 2,394,853 rows, Test: 361,248 rows, Unique users: 1,407,580\n",
      "Building item category matrix...\n",
      "Items in train: 229,547\n",
      "Items in features: 417,053\n",
      "Common items after sync: 180,664\n",
      "Building CB embeddings (TruncatedSVD) ...\n",
      "Building HNSW index on CB embeddings...\n",
      "Building item-user sparse matrix...\n",
      "Items in CF: 180,664\n",
      "Items in CB: 180,664\n",
      "Final common items (CF âˆ© CB): 180,664\n",
      "Building item neighbors (item-based CF) ... (may take time)\n",
      "ðŸŸ¢ Found existing cache: cache_reco\\item_neighbors.pkl\n",
      "âœ… Cache valid. Using cached neighbors (180,664 items, topk=200).\n",
      "item_neighbors_idx shape: (180664, 200), item_user_sparse items: 180664\n",
      "Starting evaluation of hybrid item-based CF + CB ...\n",
      "Evaluating hybrid item-based CF + CB with safety checks + checkpointing...\n",
      "Total users in test: 117,268\n",
      "ðŸŸ¢ Found checkpoint: cache_reco\\hybrid_eval_checkpoint.pkl, resuming evaluation...\n",
      "Evaluating Î±=0.1 (1/5)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62f61807c1e148e5a20454dee9f47f94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/107268 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Checkpoint saved at user 11000/117268 for Î±=0.1\n",
      "ðŸ’¾ Checkpoint saved at user 12000/117268 for Î±=0.1\n",
      "ðŸ’¾ Checkpoint saved at user 13000/117268 for Î±=0.1\n",
      "ðŸ’¾ Checkpoint saved at user 14000/117268 for Î±=0.1\n",
      "ðŸ’¾ Checkpoint saved at user 15000/117268 for Î±=0.1\n",
      "ðŸ’¾ Checkpoint saved at user 16000/117268 for Î±=0.1\n",
      "ðŸ’¾ Checkpoint saved at user 17000/117268 for Î±=0.1\n",
      "ðŸ’¾ Checkpoint saved at user 18000/117268 for Î±=0.1\n",
      "ðŸ’¾ Checkpoint saved at user 19000/117268 for Î±=0.1\n",
      "ðŸ’¾ Checkpoint saved at user 20000/117268 for Î±=0.1\n",
      "ðŸ’¾ Checkpoint saved at user 21000/117268 for Î±=0.1\n",
      "ðŸ’¾ Checkpoint saved at user 22000/117268 for Î±=0.1\n",
      "ðŸ’¾ Checkpoint saved at user 23000/117268 for Î±=0.1\n",
      "ðŸ’¾ Checkpoint saved at user 24000/117268 for Î±=0.1\n",
      "ðŸ’¾ Checkpoint saved at user 25000/117268 for Î±=0.1\n",
      "ðŸ’¾ Checkpoint saved at user 26000/117268 for Î±=0.1\n",
      "ðŸ’¾ Checkpoint saved at user 27000/117268 for Î±=0.1\n",
      "ðŸ’¾ Checkpoint saved at user 28000/117268 for Î±=0.1\n",
      "ðŸ’¾ Checkpoint saved at user 29000/117268 for Î±=0.1\n",
      "ðŸ’¾ Checkpoint saved at user 30000/117268 for Î±=0.1\n",
      "ðŸ’¾ Checkpoint saved at user 31000/117268 for Î±=0.1\n",
      "ðŸ’¾ Checkpoint saved at user 32000/117268 for Î±=0.1\n",
      "ðŸ’¾ Checkpoint saved at user 33000/117268 for Î±=0.1\n",
      "ðŸ’¾ Checkpoint saved at user 34000/117268 for Î±=0.1\n",
      "ðŸ’¾ Checkpoint saved at user 35000/117268 for Î±=0.1\n",
      "ðŸ’¾ Checkpoint saved at user 36000/117268 for Î±=0.1\n",
      "ðŸ’¾ Checkpoint saved at user 37000/117268 for Î±=0.1\n",
      "ðŸ’¾ Checkpoint saved at user 38000/117268 for Î±=0.1\n",
      "ðŸ’¾ Checkpoint saved at user 39000/117268 for Î±=0.1\n",
      "ðŸ’¾ Checkpoint saved at user 40000/117268 for Î±=0.1\n",
      "ðŸ’¾ Checkpoint saved at user 41000/117268 for Î±=0.1\n",
      "ðŸ’¾ Checkpoint saved at user 42000/117268 for Î±=0.1\n",
      "ðŸ’¾ Checkpoint saved at user 43000/117268 for Î±=0.1\n",
      "ðŸ’¾ Checkpoint saved at user 44000/117268 for Î±=0.1\n",
      "ðŸ’¾ Checkpoint saved at user 45000/117268 for Î±=0.1\n",
      "ðŸ’¾ Checkpoint saved at user 46000/117268 for Î±=0.1\n",
      "ðŸ’¾ Checkpoint saved at user 47000/117268 for Î±=0.1\n",
      "ðŸ’¾ Checkpoint saved at user 48000/117268 for Î±=0.1\n",
      "ðŸ’¾ Checkpoint saved at user 49000/117268 for Î±=0.1\n",
      "ðŸ’¾ Checkpoint saved at user 50000/117268 for Î±=0.1\n",
      "ðŸ’¾ Checkpoint saved at user 51000/117268 for Î±=0.1\n",
      "ðŸ’¾ Checkpoint saved at user 52000/117268 for Î±=0.1\n",
      "ðŸ’¾ Checkpoint saved at user 53000/117268 for Î±=0.1\n",
      "ðŸ’¾ Checkpoint saved at user 54000/117268 for Î±=0.1\n",
      "ðŸ’¾ Checkpoint saved at user 55000/117268 for Î±=0.1\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
