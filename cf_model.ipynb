{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26c3663c",
   "metadata": {},
   "source": [
    "# **Load & basic preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d48e757",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from typing import List, Tuple, Dict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.sparse import csr_matrix, vstack\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import MinMaxScaler, normalize\n",
    "import hnswlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1de12241",
   "metadata": {},
   "outputs": [],
   "source": [
    "EVENTS_FILE = \"events.csv\"\n",
    "ITEM_PROP1_FILE = \"item_properties_part1.csv\"\n",
    "ITEM_PROP2_FILE = \"item_properties_part2.csv\"\n",
    "CATEGORY_TREE_FILE = \"category_tree.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1c4e9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = \"cache_reco\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a434c9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVD_DIM = 64                 # embedding dim for CB after TruncatedSVD\n",
    "ITEM_NN_TOPK = 200           # compute top-k item neighbors in item-based CF\n",
    "CF_NEIGHBORS_TOPK = 50      # when scoring per user, use top 50 similar items\n",
    "HNSW_M = 64\n",
    "HNSW_EF_CONSTRUCTION = 200\n",
    "HNSW_EF_SEARCH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cbb0b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_INTERACTIONS_ACTIVE_USER = 1  # we keep users with >= this interactions before train-test split\n",
    "TRAIN_TEST_LAST_N = 3             # last N interactions per user go to test\n",
    "ALPHAS = [0.1, 0.3, 0.5, 0.7, 0.9]  # for evaluation\n",
    "K_EVAL = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67d1553a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pickle(obj, path):\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(obj, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea47459e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8db0464d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    events = pd.read_csv(EVENTS_FILE)\n",
    "    ip1 = pd.read_csv(ITEM_PROP1_FILE)\n",
    "    ip2 = pd.read_csv(ITEM_PROP2_FILE)\n",
    "    cat = pd.read_csv(CATEGORY_TREE_FILE)\n",
    "    item_props = pd.concat([ip1, ip2], axis=0, ignore_index=True)\n",
    "    return events, item_props, cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61f9518f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_events(events: pd.DataFrame) -> pd.DataFrame:\n",
    "    # keep valid event types\n",
    "    events = events[events['event'].isin(['view', 'addtocart', 'transaction'])].copy()\n",
    "    # parse timestamp to datetime\n",
    "    if not np.issubdtype(events['timestamp'].dtype, np.datetime64):\n",
    "        events['timestamp'] = pd.to_datetime(events['timestamp'], unit='ms', errors='coerce')\n",
    "    # map event type to weights (implicit feedback)\n",
    "    weight_map = {'view': 1, 'addtocart': 4, 'transaction': 10}\n",
    "    events['weight'] = events['event'].map(weight_map)\n",
    "    # sort by visitor + timestamp globally for stable later slicing\n",
    "    events = events.sort_values(['visitorid', 'timestamp']).reset_index(drop=True)\n",
    "    return events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b33e7dd",
   "metadata": {},
   "source": [
    "# **Train/Test split by last-N per user**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35fb9a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_by_last_n(events: pd.DataFrame, n_last=3) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    train_rows = []\n",
    "    test_rows = []\n",
    "\n",
    "    for visitor, g in events.groupby('visitorid'):\n",
    "        g = g.sort_values('timestamp')\n",
    "        if len(g) > n_last:\n",
    "            train_rows.append(g.iloc[:-n_last])\n",
    "            test_rows.append(g.iloc[-n_last:])\n",
    "        else:\n",
    "            train_rows.append(g)\n",
    "            # no test rows for low-interaction users\n",
    "    train_df = pd.concat(train_rows).reset_index(drop=True)\n",
    "    test_df = pd.concat(test_rows).reset_index(drop=True) if test_rows else pd.DataFrame(columns=events.columns)\n",
    "    print(f\"Train: {len(train_df):,} rows, Test: {len(test_df):,} rows, Unique users: {events['visitorid'].nunique():,}\")\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1ee3fb",
   "metadata": {},
   "source": [
    "# **Build item features matrix (one-hot categories) then SVD -> dense embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ece12e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_item_category_matrix(item_props: pd.DataFrame, category_tree: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Filter category properties: property == 'categoryid'\n",
    "    cats = item_props[item_props['property'] == 'categoryid'][['itemid', 'value']].copy()\n",
    "    cats.rename(columns={'value': 'categoryid'}, inplace=True)\n",
    "    cats['categoryid'] = cats['categoryid'].astype(int)\n",
    "    # Expand via category_tree parents\n",
    "    ct = category_tree.copy()\n",
    "    ct['parentid'] = ct['parentid'].fillna(0).astype(int)\n",
    "    ct['categoryid'] = ct['categoryid'].astype(int)\n",
    "    parent_map = dict(zip(ct['categoryid'], ct['parentid']))\n",
    "\n",
    "    def get_parents(cat_id):\n",
    "        res = []\n",
    "        seen = set()\n",
    "        while cat_id in parent_map and parent_map[cat_id] != 0 and cat_id not in seen:\n",
    "            seen.add(cat_id)\n",
    "            cat_id = parent_map[cat_id]\n",
    "            res.append(cat_id)\n",
    "        return res\n",
    "\n",
    "    extra_rows = []\n",
    "    for _, r in cats.iterrows():\n",
    "        parents = get_parents(r['categoryid'])\n",
    "        for p in parents:\n",
    "            extra_rows.append({'itemid': r['itemid'], 'categoryid': p})\n",
    "\n",
    "    if extra_rows:\n",
    "        cats = pd.concat([cats, pd.DataFrame(extra_rows)], ignore_index=True).drop_duplicates()\n",
    "\n",
    "    # pivot one-hot\n",
    "    item_features = pd.get_dummies(cats.set_index('itemid')['categoryid']).groupby(level=0).max()\n",
    "    return item_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "013e6908",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cb_embeddings(item_features: pd.DataFrame, svd_dim: int = SVD_DIM) -> Tuple[np.ndarray, dict, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Input: item_features: DataFrame index=itemid, columns=category ids (one-hot)\n",
    "    Output: embeddings (n_items, svd_dim), itemid->idx map, idx_to_itemid array\n",
    "    \"\"\"\n",
    "    itemids = item_features.index.to_list()\n",
    "    itemid_to_idx = {iid: i for i, iid in enumerate(itemids)}\n",
    "    idx_to_itemid = np.array(itemids)\n",
    "\n",
    "    # TruncatedSVD on sparse input to get dense embeddings\n",
    "    mat = csr_matrix(item_features.values)\n",
    "    svd = TruncatedSVD(n_components=min(svd_dim, mat.shape[1]-1 if mat.shape[1]>1 else 1), random_state=42)\n",
    "    emb = svd.fit_transform(mat)  # shape (n_items, svd_dim or less)\n",
    "    # normalize embeddings (important for cosine/HNSW)\n",
    "    emb = normalize(emb)\n",
    "    return emb.astype('float32'), itemid_to_idx, idx_to_itemid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475c1721",
   "metadata": {},
   "source": [
    "# **Build HNSW index for CB embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "debabb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hnsw_index(emb: np.ndarray, ef_construction=HNSW_EF_CONSTRUCTION, M=HNSW_M, ef_search=HNSW_EF_SEARCH):\n",
    "    n_items, dim = emb.shape\n",
    "    idx = hnswlib.Index(space='cosine', dim=dim)\n",
    "    idx.init_index(max_elements=n_items, ef_construction=ef_construction, M=M)\n",
    "    ids = np.arange(n_items)\n",
    "    idx.add_items(emb, ids)\n",
    "    idx.set_ef(ef_search)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34849a3c",
   "metadata": {},
   "source": [
    "# **Item-based CF: build item-user sparse matrix and nearest neighbors (top-k)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c1263ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_item_user_matrix(train_df: pd.DataFrame) -> Tuple[csr_matrix, np.ndarray, Dict[int,int], Dict[int,int]]:\n",
    "    \"\"\"\n",
    "    Build item_user_sparse (n_items x n_users) for neighbors calculation.\n",
    "    Returns sparse matrix, itemids array, itemid->col_idx, userids->row_idx\n",
    "    \"\"\"\n",
    "    # Map users and items to indices\n",
    "    users = train_df['visitorid'].unique()\n",
    "    items = train_df['itemid'].unique()\n",
    "    user_to_idx = {u: i for i, u in enumerate(users)}\n",
    "    item_to_idx = {it: i for i, it in enumerate(items)}\n",
    "    idx_to_item = np.array(items)\n",
    "    idx_to_user = np.array(users)\n",
    "\n",
    "    # Build COO arrays\n",
    "    rows = train_df['itemid'].map(item_to_idx).to_numpy()\n",
    "    cols = train_df['visitorid'].map(user_to_idx).to_numpy()\n",
    "    data = train_df['weight'].to_numpy(dtype='float32')\n",
    "\n",
    "    # item x user\n",
    "    mat = csr_matrix((data, (rows, cols)), shape=(len(items), len(users)))\n",
    "    return mat, idx_to_item, item_to_idx, user_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ceda6ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_item_neighbors(item_user_sparse: csr_matrix, topk=ITEM_NN_TOPK, cache_path=None):\n",
    "    \"\"\"\n",
    "    Build top-k item neighbors using NearestNeighbors on sparse rows (items).\n",
    "    Returns neighbors indices and distances arrays aligned with item index.\n",
    "    \"\"\"\n",
    "    if cache_path and os.path.exists(cache_path):\n",
    "        print(\"Loading item neighbors from cache:\", cache_path)\n",
    "        return load_pickle(cache_path)\n",
    "\n",
    "    # NearestNeighbors on sparse matrix; using metric='cosine' yields distances (0..2)\n",
    "    nn = NearestNeighbors(n_neighbors=min(topk, item_user_sparse.shape[0]), metric='cosine', algorithm='brute', n_jobs=-1)\n",
    "    nn.fit(item_user_sparse)\n",
    "    distances, indices = nn.kneighbors(item_user_sparse, return_distance=True)\n",
    "    # convert distances -> similarities (cosine similarity): sim = 1 - dist\n",
    "    sims = 1.0 - distances\n",
    "    if cache_path:\n",
    "        save_pickle((indices, sims), cache_path)\n",
    "    return indices, sims"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c35877e",
   "metadata": {},
   "source": [
    "# **Recommendation functions (CF, CB, Hybrid)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb98d3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_item_based_cf_scores_for_user(user_id: int, user_item_matrix_useridx_map: Dict[int,int],\n",
    "                                      user_item_matrix: csr_matrix,\n",
    "                                      item_neighbors_idx: np.ndarray, item_neighbors_sims: np.ndarray,\n",
    "                                      topk_scores=CF_NEIGHBORS_TOPK) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Compute CF item scores for a user by aggregating similarities from items the user interacted with.\n",
    "    user_item_matrix: item x user sparse matrix\n",
    "    item_neighbors_idx/sims: arrays shape (n_items, k)\n",
    "    Returns pd.Series indexed by itemid (string/int).\n",
    "    \"\"\"\n",
    "    # map user -> column index\n",
    "    if user_id not in user_item_matrix_useridx_map:\n",
    "        return pd.Series(dtype=float)\n",
    "\n",
    "    uidx = user_item_matrix_useridx_map[user_id]\n",
    "    # get user's interacted items (nonzero rows in column uidx)\n",
    "    user_col = user_item_matrix[:, uidx].toarray().flatten()  # length n_items\n",
    "    interacted_indices = np.where(user_col > 0)[0]\n",
    "    if len(interacted_indices) == 0:\n",
    "        return pd.Series(dtype=float)\n",
    "\n",
    "    n_items = item_neighbors_idx.shape[0]\n",
    "    agg_scores = np.zeros(n_items, dtype='float32')\n",
    "\n",
    "    # For each interacted item, add neighbor similarities\n",
    "    for it_idx in interacted_indices:\n",
    "        neigh_idxs = item_neighbors_idx[it_idx]\n",
    "        neigh_sims = item_neighbors_sims[it_idx]\n",
    "        # accumulate weighted by user's interaction weight\n",
    "        w = user_col[it_idx]\n",
    "        agg_scores[neigh_idxs] += (neigh_sims * w)\n",
    "\n",
    "    # zero out already-seen items\n",
    "    agg_scores[interacted_indices] = 0.0\n",
    "    # return as Series indexed by item index (we'll map to itemids outside)\n",
    "    return pd.Series(agg_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7caaf055",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cb_scores_for_user_by_last_item(user_id: int, user_item_matrix_useridx_map: Dict[int,int],\n",
    "                                        user_item_matrix: csr_matrix,\n",
    "                                        itemid_to_emb_idx: Dict[int,int],\n",
    "                                        emb_index: hnswlib.Index,\n",
    "                                        emb_data: np.ndarray,\n",
    "                                        idx_to_itemid_emb: np.ndarray,\n",
    "                                        top_n=100) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Uses last item the user interacted with as anchor, query HNSW for similar items and return score series.\n",
    "    \"\"\"\n",
    "    if user_id not in user_item_matrix_useridx_map:\n",
    "        return pd.Series(dtype=float)\n",
    "\n",
    "    uidx = user_item_matrix_useridx_map[user_id]\n",
    "    user_col = user_item_matrix[:, uidx].toarray().flatten()\n",
    "    interacted_indices = np.where(user_col > 0)[0]\n",
    "    if len(interacted_indices) == 0:\n",
    "        return pd.Series(dtype=float)\n",
    "\n",
    "    last_idx = interacted_indices[-1]  # index in item_user matrix\n",
    "    # convert this index (in item_user matrix) to itemid then to emb idx if mapping exists\n",
    "    # Note: We'll provide a mapping outside to link item_user index -> emb index\n",
    "    # Here assume item_user index aligns with idx_to_itemid_emb mapping (we'll create that mapping earlier)\n",
    "    if last_idx not in itemid_to_emb_idx:\n",
    "        # fallback: empty\n",
    "        return pd.Series(dtype=float)\n",
    "\n",
    "    emb_idx = itemid_to_emb_idx[last_idx]\n",
    "    k = min(top_n + 1, emb_data.shape[0])\n",
    "    labels, distances = emb_index.knn_query(emb_data[emb_idx], k=k)\n",
    "    labels = labels[0]\n",
    "    distances = distances[0]\n",
    "    # skip self\n",
    "    if len(labels) > 1:\n",
    "        labels = labels[1:]\n",
    "        distances = distances[1:]\n",
    "    sims = 1.0 - distances  # convert dist->sim\n",
    "    # map emb idx back to item_user idx (via idx_to_itemid_emb -> itemid -> item_user index)\n",
    "    items_emb_ids = idx_to_itemid_emb[labels]\n",
    "    scores = pd.Series(sims, index=items_emb_ids)\n",
    "    return scores  # index are item_user idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f16cfadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_cf_cb_scores(cf_scores: pd.Series, cb_scores: pd.Series, alpha=0.7):\n",
    "    \"\"\"\n",
    "    cf_scores and cb_scores are Series indexed by same item index space (build union).\n",
    "    alpha = weight for CF (0..1), (1-alpha) for CB.\n",
    "    Returns Series of hybrid scores.\n",
    "    \"\"\"\n",
    "    if cf_scores.empty and cb_scores.empty:\n",
    "        return pd.Series(dtype=float)\n",
    "    # align union\n",
    "    union_idx = cf_scores.index.union(cb_scores.index)\n",
    "    cf_aligned = cf_scores.reindex(union_idx).fillna(0.0)\n",
    "    cb_aligned = cb_scores.reindex(union_idx).fillna(0.0)\n",
    "    # normalize each to [0,1]\n",
    "    def minmax(s):\n",
    "        if s.max() - s.min() < 1e-9:\n",
    "            return pd.Series(0.0, index=s.index)\n",
    "        return (s - s.min()) / (s.max() - s.min())\n",
    "    cf_norm = minmax(cf_aligned)\n",
    "    cb_norm = minmax(cb_aligned)\n",
    "    hybrid = alpha * cf_norm + (1 - alpha) * cb_norm\n",
    "    hybrid = hybrid.sort_values(ascending=False)\n",
    "    return hybrid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f00d28",
   "metadata": {},
   "source": [
    "# **Evaluation metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d31c197",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_at_k(recommended: List[int], true_items: List[int], k=5) -> Tuple[float,float]:\n",
    "    rec = recommended[:k]\n",
    "    if len(rec) == 0:\n",
    "        return 0.0, 0.0\n",
    "    hits = len(set(rec) & set(true_items))\n",
    "    precision = hits / k\n",
    "    recall = hits / len(true_items) if len(true_items) > 0 else 0.0\n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53567c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndcg_at_k(recommended: List[int], true_items: List[int], k=5) -> float:\n",
    "    dcg = 0.0\n",
    "    for i, r in enumerate(recommended[:k]):\n",
    "        if r in true_items:\n",
    "            dcg += 1.0 / np.log2(i + 2)\n",
    "    ideal = sum(1.0/np.log2(i+2) for i in range(min(len(true_items), k)))\n",
    "    return dcg / ideal if ideal > 0 else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dad41f",
   "metadata": {},
   "source": [
    "# **Full evaluation loop (item-based CF + CB hybrid)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d47898e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_hybrid_item_cf(\n",
    "    train_df, test_df, item_features,\n",
    "    emb, itemuser_idx_to_emb_idx, idx_to_itemid_emb_arr, emb_index,\n",
    "    item_user_sparse, idx_to_itemid_itemuser,\n",
    "    item_neighbors_idx, item_neighbors_sims,\n",
    "    alpha_values, k=5\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluasi hybrid item-based CF + CB dengan proteksi mismatch index.\n",
    "    \"\"\"\n",
    "    print(\"Evaluating hybrid item-based CF + CB with safety checks...\")\n",
    "\n",
    "    # Bangun mapping cepat: item_user_idx -> itemid\n",
    "    itemuser_idx_to_itemid = np.array(idx_to_itemid_itemuser)\n",
    "\n",
    "    # Precompute ground truth per user\n",
    "    true_items_per_user = (\n",
    "        test_df.groupby(\"visitorid\")[\"itemid\"].apply(list).to_dict()\n",
    "    )\n",
    "\n",
    "    results = []\n",
    "    total_users = len(true_items_per_user)\n",
    "    print(f\"Total users in test: {total_users}\")\n",
    "\n",
    "    # Loop untuk setiap alpha\n",
    "    for alpha in alpha_values:\n",
    "        print(f\"Evaluating α={alpha}...\")\n",
    "        precisions, recalls, hit_rates, ndcgs = [], [], [], []\n",
    "        all_recommended_items = set()\n",
    "\n",
    "        # Evaluasi per user\n",
    "        for u, true_items in tqdm(true_items_per_user.items(), total=total_users):\n",
    "            # Ambil semua item yang pernah diinteraksikan user ini (di train)\n",
    "            user_train_items = train_df[train_df[\"visitorid\"] == u][\"itemid\"].tolist()\n",
    "            if not user_train_items:\n",
    "                continue\n",
    "\n",
    "            # === (1) Ambil CF rekomendasi ===\n",
    "            cf_scores = {}\n",
    "            for itemid in user_train_items:\n",
    "                if itemid not in idx_to_itemid_itemuser:\n",
    "                    continue\n",
    "                try:\n",
    "                    item_idx = idx_to_itemid_itemuser.index(itemid)\n",
    "                    neigh_idxs = item_neighbors_idx[item_idx]\n",
    "                    neigh_sims = item_neighbors_sims[item_idx]\n",
    "                    for ni, sim in zip(neigh_idxs, neigh_sims):\n",
    "                        if ni < len(idx_to_itemid_itemuser):  # proteksi\n",
    "                            cf_scores[ni] = cf_scores.get(ni, 0) + sim\n",
    "                except Exception:\n",
    "                    continue\n",
    "            if not cf_scores:\n",
    "                continue\n",
    "\n",
    "            # Konversi ke pandas Series\n",
    "            cf_scores = pd.Series(cf_scores)\n",
    "\n",
    "            # === (2) Ambil CB rekomendasi ===\n",
    "            cb_scores = {}\n",
    "            last_item = user_train_items[-1]\n",
    "            # Dapatkan embedding index untuk item ini\n",
    "            emb_idx = None\n",
    "            for k_it, v in itemuser_idx_to_emb_idx.items():\n",
    "                if idx_to_itemid_itemuser[k_it] == last_item:\n",
    "                    emb_idx = v\n",
    "                    break\n",
    "            if emb_idx is not None:\n",
    "                try:\n",
    "                    labels, distances = emb_index.knn_query(\n",
    "                        emb[emb_idx], k=min(200, len(emb))\n",
    "                    )\n",
    "                    cb_items = labels[0]\n",
    "                    cb_dists = 1 - distances[0]\n",
    "                    for emb_i, dist in zip(cb_items, cb_dists):\n",
    "                        if emb_i < len(idx_to_itemid_emb_arr):\n",
    "                            itemid = idx_to_itemid_emb_arr[emb_i]\n",
    "                            if itemid in idx_to_itemid_itemuser:\n",
    "                                cf_idx = idx_to_itemid_itemuser.index(itemid)\n",
    "                                cb_scores[cf_idx] = dist\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            cb_scores = pd.Series(cb_scores) if cb_scores else pd.Series(dtype=float)\n",
    "\n",
    "            # === (3) Gabungkan CF + CB ===\n",
    "            cf_scaled = (cf_scores - cf_scores.min()) / (cf_scores.max() - cf_scores.min() + 1e-9)\n",
    "            cb_scaled = (cb_scores - cb_scores.min()) / (cb_scores.max() - cb_scores.min() + 1e-9)\n",
    "            hybrid = alpha * cf_scaled.add((1 - alpha) * cb_scaled, fill_value=0)\n",
    "            hybrid = hybrid.drop(\n",
    "                [idx_to_itemid_itemuser.index(i) for i in user_train_items if i in idx_to_itemid_itemuser],\n",
    "                errors=\"ignore\",\n",
    "            ).sort_values(ascending=False)\n",
    "\n",
    "            rec_itemuser_idxs = hybrid.index[:k].tolist()\n",
    "            # ✅ Aman: filter hanya index valid\n",
    "            rec_itemuser_idxs = [idx for idx in rec_itemuser_idxs if idx < len(itemuser_idx_to_itemid)]\n",
    "            rec_itemids = [itemuser_idx_to_itemid[idx] for idx in rec_itemuser_idxs if idx < len(itemuser_idx_to_itemid)]\n",
    "\n",
    "            if not rec_itemids:\n",
    "                continue\n",
    "\n",
    "            # === (4) Hitung metrik evaluasi ===\n",
    "            hits = len(set(rec_itemids) & set(true_items))\n",
    "            precisions.append(hits / k)\n",
    "            recalls.append(hits / len(true_items))\n",
    "            hit_rates.append(1 if hits > 0 else 0)\n",
    "            all_recommended_items.update(rec_itemids)\n",
    "\n",
    "            # NDCG\n",
    "            dcg = sum(1 / np.log2(i + 2) for i, rec in enumerate(rec_itemids) if rec in true_items)\n",
    "            ideal_dcg = sum(1 / np.log2(i + 2) for i in range(min(len(true_items), k)))\n",
    "            ndcgs.append(dcg / ideal_dcg if ideal_dcg > 0 else 0)\n",
    "\n",
    "        # === (5) Simpan hasil ===\n",
    "        results.append({\n",
    "            \"alpha\": alpha,\n",
    "            \"Precision@5\": np.mean(precisions) if precisions else 0,\n",
    "            \"Recall@5\": np.mean(recalls) if recalls else 0,\n",
    "            \"HitRate@5\": np.mean(hit_rates) if hit_rates else 0,\n",
    "            \"Coverage\": len(all_recommended_items) / len(item_features) if len(item_features) > 0 else 0,\n",
    "            \"NDCG@5\": np.mean(ndcgs) if ndcgs else 0\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d44b65c",
   "metadata": {},
   "source": [
    "# **Orchestrator main()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e3d985b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"Loading data...\")\n",
    "    events, item_props, category_tree = load_data()\n",
    "    events = preprocess_events(events)\n",
    "\n",
    "    # Filter user aktif\n",
    "    events = events[events['visitorid'].notnull()].copy()\n",
    "    user_counts = events['visitorid'].value_counts()\n",
    "    active_users_list = user_counts[user_counts >= MIN_INTERACTIONS_ACTIVE_USER].index\n",
    "    events = events[events['visitorid'].isin(active_users_list)].copy()\n",
    "    print(f\"After filtering active users: total interactions {len(events):,}, unique users {events['visitorid'].nunique():,}\")\n",
    "\n",
    "    print(\"Train/test split (last N per user)...\")\n",
    "    train_df, test_df = train_test_split_by_last_n(events, n_last=TRAIN_TEST_LAST_N)\n",
    "\n",
    "    # Build item feature matrix (content-based)\n",
    "    print(\"Building item category matrix...\")\n",
    "    item_features = build_item_category_matrix(item_props, category_tree)\n",
    "\n",
    "    # ✅ Sinkronisasi item agar CF dan CB memiliki domain yang sama\n",
    "    train_items = set(train_df['itemid'].unique())\n",
    "    feature_items = set(item_features.index)\n",
    "    common_items = list(train_items & feature_items)\n",
    "\n",
    "    print(f\"Items in train: {len(train_items):,}\")\n",
    "    print(f\"Items in features: {len(feature_items):,}\")\n",
    "    print(f\"Common items after sync: {len(common_items):,}\")\n",
    "\n",
    "    # Filter hanya item yang muncul di keduanya\n",
    "    train_df = train_df[train_df['itemid'].isin(common_items)].copy()\n",
    "    test_df = test_df[test_df['itemid'].isin(common_items)].copy()\n",
    "    item_features = item_features.loc[common_items].copy()\n",
    "\n",
    "    # Bangun embedding CB\n",
    "    print(\"Building CB embeddings (TruncatedSVD) ...\")\n",
    "    emb, itemid_to_emb_idx_map, idx_to_itemid_emb_arr = build_cb_embeddings(item_features, svd_dim=SVD_DIM)\n",
    "\n",
    "    # Bangun index HNSW untuk CB\n",
    "    print(\"Building HNSW index on CB embeddings...\")\n",
    "    emb_index = build_hnsw_index(emb, ef_construction=HNSW_EF_CONSTRUCTION, M=HNSW_M, ef_search=HNSW_EF_SEARCH)\n",
    "\n",
    "    # Bangun matriks item–user untuk CF (item-based)\n",
    "    print(\"Building item-user sparse matrix...\")\n",
    "    item_user_sparse, idx_to_itemid_itemuser, item_to_idx_itemuser, user_to_idx_itemuser = build_item_user_matrix(train_df)\n",
    "\n",
    "    # ✅ Sinkronisasi lagi: pastikan item_user dan CB sama-sama mengenal item\n",
    "    valid_items_cf = set(idx_to_itemid_itemuser)\n",
    "    valid_items_cb = set(itemid_to_emb_idx_map.keys())\n",
    "    common_final_items = list(valid_items_cf & valid_items_cb)\n",
    "    print(f\"Items in CF: {len(valid_items_cf):,}\")\n",
    "    print(f\"Items in CB: {len(valid_items_cb):,}\")\n",
    "    print(f\"Final common items (CF ∩ CB): {len(common_final_items):,}\")\n",
    "\n",
    "    # Filter kembali matriks CF agar hanya mencakup item yang ada di kedua sisi\n",
    "    keep_idx = [i for i, iid in enumerate(idx_to_itemid_itemuser) if iid in common_final_items]\n",
    "    item_user_sparse = item_user_sparse[keep_idx, :]\n",
    "    idx_to_itemid_itemuser = [idx_to_itemid_itemuser[i] for i in keep_idx]\n",
    "\n",
    "    # Mapping item_user idx ↔ emb idx\n",
    "    itemuser_idx_to_emb_idx = {}\n",
    "    for it_idx, itemid in enumerate(idx_to_itemid_itemuser):\n",
    "        if itemid in itemid_to_emb_idx_map:\n",
    "            itemuser_idx_to_emb_idx[it_idx] = itemid_to_emb_idx_map[itemid]\n",
    "\n",
    "    # Reverse mapping emb idx → item_user idx\n",
    "    emb_idx_to_itemuser_idx = {}\n",
    "    itemid_to_itemuser_idx = {itemid: idx for idx, itemid in enumerate(idx_to_itemid_itemuser)}\n",
    "    for emb_idx, itemid in enumerate(idx_to_itemid_emb_arr):\n",
    "        if itemid in itemid_to_itemuser_idx:\n",
    "            emb_idx_to_itemuser_idx[emb_idx] = itemid_to_itemuser_idx[itemid]\n",
    "\n",
    "    # Bangun tetangga item (item-based CF)\n",
    "    neighbors_cache = os.path.join(CACHE_DIR, \"item_neighbors.pkl\")\n",
    "    print(\"Building item neighbors (item-based CF) ... (may take time)\")\n",
    "    item_neighbors_idx, item_neighbors_sims = build_item_neighbors(item_user_sparse, topk=ITEM_NN_TOPK, cache_path=neighbors_cache)\n",
    "\n",
    "    # ✅ Proteksi tambahan terhadap mismatch indeks\n",
    "    print(f\"item_neighbors_idx shape: {item_neighbors_idx.shape}, item_user_sparse items: {len(idx_to_itemid_itemuser)}\")\n",
    "\n",
    "    # Evaluasi hybrid (CF + CB)\n",
    "    print(\"Starting evaluation of hybrid item-based CF + CB ...\")\n",
    "    results_df = evaluate_hybrid_item_cf(\n",
    "        train_df, test_df, item_features,\n",
    "        emb, itemuser_idx_to_emb_idx, idx_to_itemid_emb_arr, emb_index,\n",
    "        item_user_sparse, idx_to_itemid_itemuser,\n",
    "        item_neighbors_idx, item_neighbors_sims,\n",
    "        alpha_values=ALPHAS, k=K_EVAL\n",
    "    )\n",
    "\n",
    "    print(\"\\n===== Evaluation Results =====\")\n",
    "    print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fbaebc07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "After filtering active users: total interactions 2,756,101, unique users 1,407,580\n",
      "Train/test split (last N per user)...\n",
      "Train: 2,394,853 rows, Test: 361,248 rows, Unique users: 1,407,580\n",
      "Building item category matrix...\n",
      "Items in train: 229,547\n",
      "Items in features: 417,053\n",
      "Common items after sync: 180,664\n",
      "Building CB embeddings (TruncatedSVD) ...\n",
      "Building HNSW index on CB embeddings...\n",
      "Building item-user sparse matrix...\n",
      "Items in CF: 180,664\n",
      "Items in CB: 180,664\n",
      "Final common items (CF ∩ CB): 180,664\n",
      "Building item neighbors (item-based CF) ... (may take time)\n",
      "Loading item neighbors from cache: cache_reco\\item_neighbors.pkl\n",
      "item_neighbors_idx shape: (229547, 200), item_user_sparse items: 180664\n",
      "Starting evaluation of hybrid item-based CF + CB ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "547c925dcd424feea047acc13b738436",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval α=0.1:   0%|          | 0/117268 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[22], line 82\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# Evaluasi hybrid (CF + CB)\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting evaluation of hybrid item-based CF + CB ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 82\u001b[0m results_df \u001b[38;5;241m=\u001b[39m evaluate_hybrid_item_cf(\n\u001b[0;32m     83\u001b[0m     train_df, test_df, item_features,\n\u001b[0;32m     84\u001b[0m     emb, itemuser_idx_to_emb_idx, idx_to_itemid_emb_arr, emb_index,\n\u001b[0;32m     85\u001b[0m     item_user_sparse, idx_to_itemid_itemuser,\n\u001b[0;32m     86\u001b[0m     item_neighbors_idx, item_neighbors_sims,\n\u001b[0;32m     87\u001b[0m     alpha_values\u001b[38;5;241m=\u001b[39mALPHAS, k\u001b[38;5;241m=\u001b[39mK_EVAL\n\u001b[0;32m     88\u001b[0m )\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m===== Evaluation Results =====\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28mprint\u001b[39m(results_df\u001b[38;5;241m.\u001b[39mto_string(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "Cell \u001b[1;32mIn[21], line 50\u001b[0m, in \u001b[0;36mevaluate_hybrid_item_cf\u001b[1;34m(train_df, test_df, item_features, emb, itemid_to_emb_idx_map, idx_to_itemid_emb_arr, emb_index, item_user_sparse, idx_to_itemid_itemuser, item_neighbors_idx, item_neighbors_sims, alpha_values, k)\u001b[0m\n\u001b[0;32m     48\u001b[0m     rec_itemuser_idxs \u001b[38;5;241m=\u001b[39m hybrid\u001b[38;5;241m.\u001b[39mindex[:k]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# map item_user idx -> itemid\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m rec_itemids \u001b[38;5;241m=\u001b[39m [itemuser_idx_to_itemid[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m rec_itemuser_idxs]\n\u001b[0;32m     51\u001b[0m true_itemids \u001b[38;5;241m=\u001b[39m true_items_per_user\u001b[38;5;241m.\u001b[39mget(u, [])\n\u001b[0;32m     52\u001b[0m p, r \u001b[38;5;241m=\u001b[39m precision_recall_at_k(rec_itemids, true_itemids, k\u001b[38;5;241m=\u001b[39mk)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
