{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8a092d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from typing import List, Tuple, Dict\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import normalize\n",
    "import hnswlib\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4653b55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "EVENTS_FILE = \"events.csv\"\n",
    "ITEM_PROP1_FILE = \"item_properties_part1.csv\"\n",
    "ITEM_PROP2_FILE = \"item_properties_part2.csv\"\n",
    "CATEGORY_TREE_FILE = \"category_tree.csv\"\n",
    "CACHE_DIR = \"cache_reco_2\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2a918128",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVD_DIM = 64\n",
    "ITEM_NN_TOPK = 200\n",
    "CF_NEIGHBORS_TOPK = 50\n",
    "HNSW_M = 64\n",
    "HNSW_EF_CONSTRUCTION = 200\n",
    "HNSW_EF_SEARCH = 100\n",
    "MIN_INTERACTIONS_ACTIVE_USER = 1\n",
    "TRAIN_TEST_LAST_N = 3\n",
    "K_EVAL = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9b57f9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pickle(obj, path):\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(obj, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "99838999",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a152ea49",
   "metadata": {},
   "source": [
    "# **Data Loading & Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "012bf75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    events = pd.read_csv(EVENTS_FILE)\n",
    "    ip1 = pd.read_csv(ITEM_PROP1_FILE)\n",
    "    ip2 = pd.read_csv(ITEM_PROP2_FILE)\n",
    "    cat = pd.read_csv(CATEGORY_TREE_FILE)\n",
    "    item_props = pd.concat([ip1, ip2], axis=0, ignore_index=True)\n",
    "    return events, item_props, cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8c3b1c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_events(events: pd.DataFrame) -> pd.DataFrame:\n",
    "    events = events[events['event'].isin(['view', 'addtocart', 'transaction'])].copy()\n",
    "    if not np.issubdtype(events['timestamp'].dtype, np.datetime64):\n",
    "        events['timestamp'] = pd.to_datetime(events['timestamp'], unit='ms', errors='coerce')\n",
    "    weight_map = {'view': 1.0, 'addtocart': 3.0, 'transaction': 5.0}\n",
    "    events['weight'] = events['event'].map(weight_map)\n",
    "    events = events.dropna(subset=['visitorid', 'itemid'])\n",
    "    events['visitorid'] = events['visitorid'].astype(int)\n",
    "    events['itemid'] = events['itemid'].astype(int)\n",
    "    return events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5e0a7629",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_by_last_n(events: pd.DataFrame, n_last=3) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    train_rows, test_rows = [], []\n",
    "    for visitor, g in events.groupby('visitorid'):\n",
    "        g = g.sort_values('timestamp')\n",
    "        if len(g) > n_last:\n",
    "            train_rows.append(g.iloc[:-n_last])\n",
    "            test_rows.append(g.iloc[-n_last:])\n",
    "        else:\n",
    "            train_rows.append(g)\n",
    "    train_df = pd.concat(train_rows)\n",
    "    test_df = pd.concat(test_rows)\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e677a7e",
   "metadata": {},
   "source": [
    "# **Content-Based Embedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1dd7748e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_item_category_matrix(item_props: pd.DataFrame, category_tree: pd.DataFrame) -> pd.DataFrame:\n",
    "    cats = item_props[item_props['property'] == 'categoryid'][['itemid', 'value']].copy()\n",
    "    cats.rename(columns={'value': 'categoryid'}, inplace=True)\n",
    "    cats['categoryid'] = cats['categoryid'].astype(int)\n",
    "    pivot = pd.get_dummies(cats.set_index('itemid')['categoryid'])\n",
    "    return pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2b14c418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cb_embeddings(item_features: pd.DataFrame, svd_dim: int = SVD_DIM):\n",
    "    itemids = item_features.index.to_list()\n",
    "    itemid_to_idx = {iid: i for i, iid in enumerate(itemids)}\n",
    "    idx_to_itemid = np.array(itemids)\n",
    "    svd = TruncatedSVD(n_components=svd_dim, random_state=42)\n",
    "    emb = svd.fit_transform(item_features.values)\n",
    "    emb = normalize(emb)\n",
    "    return emb, itemid_to_idx, idx_to_itemid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "032bec96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hnsw_index(emb: np.ndarray, ef_construction=200, M=64, ef_search=100):\n",
    "    n_items, dim = emb.shape\n",
    "    index = hnswlib.Index(space='cosine', dim=dim)\n",
    "    index.init_index(max_elements=n_items, ef_construction=ef_construction, M=M)\n",
    "    index.add_items(emb, np.arange(n_items))\n",
    "    index.set_ef(ef_search)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457323eb",
   "metadata": {},
   "source": [
    "# **Collaborative Filtering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cf0b67ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_item_user_matrix(train_df: pd.DataFrame):\n",
    "    users = train_df['visitorid'].unique()\n",
    "    items = train_df['itemid'].unique()\n",
    "    user_to_idx = {u: i for i, u in enumerate(users)}\n",
    "    item_to_idx = {it: j for j, it in enumerate(items)}\n",
    "    rows, cols, data = [], [], []\n",
    "    for _, row in train_df.iterrows():\n",
    "        rows.append(item_to_idx[row['itemid']])\n",
    "        cols.append(user_to_idx[row['visitorid']])\n",
    "        data.append(row['weight'])\n",
    "    item_user_sparse = csr_matrix((data, (rows, cols)), shape=(len(items), len(users)))\n",
    "    idx_to_itemid = [it for it in items]\n",
    "    return item_user_sparse, idx_to_itemid, item_to_idx, user_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "79dc1c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_item_neighbors(item_user_sparse, topk=200, cache_path=None):\n",
    "    if cache_path and os.path.exists(cache_path):\n",
    "        return load_pickle(cache_path)\n",
    "    model = NearestNeighbors(metric='cosine', algorithm='brute', n_jobs=-1)\n",
    "    model.fit(item_user_sparse)\n",
    "    sims, idxs = model.kneighbors(item_user_sparse, n_neighbors=topk)\n",
    "    if cache_path:\n",
    "        save_pickle((idxs, 1 - sims), cache_path)\n",
    "    return idxs, 1 - sims"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25551b44",
   "metadata": {},
   "source": [
    "# **Adaptive Alpha**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "680f5454",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_adaptive_alpha(user_id, user_item_matrix, min_alpha=0.1, max_alpha=0.9, slope='linear'):\n",
    "    if user_id not in user_item_matrix.index:\n",
    "        return min_alpha\n",
    "    n = (user_item_matrix.loc[user_id] > 0).sum()\n",
    "    if n <= 1:\n",
    "        return min_alpha\n",
    "    max_n = (user_item_matrix > 0).sum(axis=1).max()\n",
    "    frac = n / (max_n + 1e-9)\n",
    "    if slope == 'linear':\n",
    "        alpha = min_alpha + (max_alpha - min_alpha) * frac\n",
    "    elif slope == 'sqrt':\n",
    "        alpha = min_alpha + (max_alpha - min_alpha) * np.sqrt(frac)\n",
    "    else:\n",
    "        alpha = min_alpha + (max_alpha - min_alpha) * frac\n",
    "    return float(np.clip(alpha, min_alpha, max_alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deee7341",
   "metadata": {},
   "source": [
    "# **Hybrid Recommender**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ef9d72c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_weighted_recommend_adaptive(\n",
    "    user_id, train_df,\n",
    "    item_user_sparse, idx_to_itemid_itemuser,\n",
    "    item_neighbors_idx, item_neighbors_sims,\n",
    "    itemuser_idx_to_emb_idx, idx_to_itemid_emb_arr, emb_index, emb,\n",
    "    user_to_idx_itemuser,            # NEW: mapping user_id -> column index in item_user_sparse\n",
    "    top_n=5, min_alpha=0.1, max_alpha=0.9\n",
    "):\n",
    "    \"\"\"\n",
    "    Memory-safe hybrid recommender (adaptive alpha) using sparse item_user matrix.\n",
    "    - Requires: user_to_idx_itemuser mapping (visitorid -> col idx in item_user_sparse)\n",
    "    - idx_to_itemid_itemuser: list/array mapping item_user row idx -> itemid\n",
    "    - itemuser_idx_to_emb_idx: mapping item_user row idx -> emb idx (CB)\n",
    "    \"\"\"\n",
    "\n",
    "    # --- 0) user existence\n",
    "    if user_id not in user_to_idx_itemuser:\n",
    "        return []\n",
    "\n",
    "    # --- 1) compute adaptive alpha using counts from train_df (fast)\n",
    "    # use train_df counts per user (cheap)\n",
    "    n_user_inter = train_df.loc[train_df[\"visitorid\"] == user_id].shape[0]\n",
    "    if n_user_inter <= 1:\n",
    "        alpha = min_alpha\n",
    "    else:\n",
    "        max_n = train_df[\"visitorid\"].value_counts().max()\n",
    "        frac = n_user_inter / (max_n + 1e-9)\n",
    "        alpha = float(np.clip(min_alpha + (max_alpha - min_alpha) * frac, min_alpha, max_alpha))\n",
    "\n",
    "    # --- 2) get user's interacted item indices from sparse matrix\n",
    "    ucol = user_to_idx_itemuser[user_id]\n",
    "    # extract column (sparse) -> dense array of shape (n_items,)\n",
    "    user_col = item_user_sparse[:, ucol].toarray().ravel()   # safe: single column\n",
    "    interacted_item_idxs = np.where(user_col > 0)[0]\n",
    "    if len(interacted_item_idxs) == 0:\n",
    "        return []\n",
    "\n",
    "    # convert idx -> itemids for later\n",
    "    interacted_itemids = [idx_to_itemid_itemuser[i] for i in interacted_item_idxs]\n",
    "\n",
    "    # --- 3) compute CF aggregate scores (vectorized accumulation)\n",
    "    n_items = item_neighbors_idx.shape[0]\n",
    "    agg = np.zeros(n_items, dtype='float32')\n",
    "    # accumulate neighbor sims weighted by user's interaction weight\n",
    "    for it_idx in interacted_item_idxs:\n",
    "        neigh_idxs = item_neighbors_idx[it_idx]       # array length topk\n",
    "        neigh_sims = item_neighbors_sims[it_idx]      # same length\n",
    "        w = float(user_col[it_idx])\n",
    "        agg[neigh_idxs] += neigh_sims * w\n",
    "\n",
    "    # zero out already seen\n",
    "    agg[interacted_item_idxs] = 0.0\n",
    "    cf_scores_series = pd.Series(agg, index=np.arange(n_items))  # index = item_user idx\n",
    "\n",
    "    # --- 4) compute CB scores using last interacted item (via emb_index)\n",
    "    last_idx = interacted_item_idxs[-1]\n",
    "    cb_scores_series = pd.Series(dtype=float)\n",
    "    if last_idx in itemuser_idx_to_emb_idx:\n",
    "        emb_idx = itemuser_idx_to_emb_idx[last_idx]\n",
    "        kq = min(200, emb.shape[0])\n",
    "        labels, distances = emb_index.knn_query(emb[emb_idx], k=kq)\n",
    "        labels = labels[0]\n",
    "        distances = distances[0]\n",
    "        # skip self if present\n",
    "        if len(labels) > 1:\n",
    "            labels = labels[1:]\n",
    "            distances = distances[1:]\n",
    "        sims = 1.0 - distances\n",
    "        # map emb idx -> item_user idx (via idx_to_itemid_emb_arr -> itemid -> item_user idx)\n",
    "        # build mapping quickly (we assume idx_to_itemid_itemuser is list)\n",
    "        # create reverse map once outside if many calls; here do minimal operations:\n",
    "        emb_itemids = idx_to_itemid_emb_arr[labels]\n",
    "        # create index mapping emb_itemids -> item_user_idx via idx_to_itemid_itemuser.index(itemid)\n",
    "        # but using .index in loop is O(n). We'll build a dict once:\n",
    "        itemid_to_itemuser_idx = {iid: i for i, iid in enumerate(idx_to_itemid_itemuser)}\n",
    "        cb_idx = []\n",
    "        cb_vals = []\n",
    "        for iid, s in zip(emb_itemids, sims):\n",
    "            if iid in itemid_to_itemuser_idx:\n",
    "                cb_idx.append(itemid_to_itemuser_idx[iid])\n",
    "                cb_vals.append(s)\n",
    "        if cb_idx:\n",
    "            cb_scores_series = pd.Series(cb_vals, index=cb_idx)\n",
    "        else:\n",
    "            cb_scores_series = pd.Series(dtype=float)\n",
    "\n",
    "    # --- 5) normalize both series to [0,1]\n",
    "    def safe_minmax_ser(s):\n",
    "        if s.empty:\n",
    "            return s\n",
    "        a = s.to_numpy(dtype='float32')\n",
    "        mn, mx = a.min(), a.max()\n",
    "        if mx - mn < 1e-9:\n",
    "            return pd.Series(0.0, index=s.index)\n",
    "        return pd.Series((a - mn) / (mx - mn), index=s.index)\n",
    "\n",
    "    cf_norm = safe_minmax_ser(cf_scores_series)\n",
    "    cb_norm = safe_minmax_ser(cb_scores_series)\n",
    "\n",
    "    # union indices -> item_user idx space\n",
    "    cf_norm = cf_norm.groupby(cf_norm.index).mean()\n",
    "    cb_norm = cb_norm.groupby(cb_norm.index).mean()\n",
    "    \n",
    "    union_idx = cf_norm.index.union(cb_norm.index)\n",
    "\n",
    "    cf_al = cf_norm.reindex(union_idx).fillna(0.0)\n",
    "    cb_al = cb_norm.reindex(union_idx).fillna(0.0)\n",
    "\n",
    "\n",
    "    hybrid = alpha * cf_al + (1 - alpha) * cb_al\n",
    "    # drop seen items\n",
    "    hybrid = hybrid.drop(interacted_item_idxs, errors='ignore')\n",
    "    # take top_n\n",
    "    top_idxs = hybrid.sort_values(ascending=False).head(top_n).index.tolist()\n",
    "    rec_itemids = [idx_to_itemid_itemuser[i] for i in top_idxs if i < len(idx_to_itemid_itemuser)]\n",
    "    return rec_itemids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b712cb4",
   "metadata": {},
   "source": [
    "# **Evaluation Adaptive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a11fa802",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_at_k(recommended_items, true_items, k=10):\n",
    "    \"\"\"\n",
    "    Hitung precision dan recall pada top-k rekomendasi.\n",
    "    \"\"\"\n",
    "    if not recommended_items or not true_items:\n",
    "        return 0.0, 0.0\n",
    "\n",
    "    recommended_at_k = recommended_items[:k]\n",
    "    true_set = set(true_items)\n",
    "\n",
    "    hits = len(set(recommended_at_k) & true_set)\n",
    "    precision = hits / k\n",
    "    recall = hits / len(true_set) if len(true_set) > 0 else 0.0\n",
    "\n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9345b333",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_at_k(recommended_items, true_items, k=10):\n",
    "    \"\"\"\n",
    "    Hitung Discounted Cumulative Gain (DCG) pada top-k rekomendasi.\n",
    "    \"\"\"\n",
    "    recommended_at_k = recommended_items[:k]\n",
    "    true_set = set(true_items)\n",
    "    dcg = 0.0\n",
    "    for i, item in enumerate(recommended_at_k):\n",
    "        if item in true_set:\n",
    "            dcg += 1.0 / np.log2(i + 2)  # posisi dimulai dari 1\n",
    "    return dcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ebde2a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndcg_at_k(recommended_items, true_items, k=10):\n",
    "    \"\"\"\n",
    "    Hitung Normalized DCG (NDCG) pada top-k rekomendasi.\n",
    "    \"\"\"\n",
    "    ideal_dcg = dcg_at_k(true_items, true_items, k)\n",
    "    if ideal_dcg == 0:\n",
    "        return 0.0\n",
    "    return dcg_at_k(recommended_items, true_items, k) / ideal_dcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e6d22732",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_hybrid_item_cf_adaptive(\n",
    "    train_df, test_df, item_features,\n",
    "    emb, itemuser_idx_to_emb_idx, idx_to_itemid_emb_arr, emb_index,\n",
    "    item_user_sparse, idx_to_itemid_itemuser,\n",
    "    item_neighbors_idx, item_neighbors_sims,\n",
    "    user_to_idx_itemuser,\n",
    "    min_alpha=0.1, max_alpha=0.9, k=5,\n",
    "    max_eval_users=10000, checkpoint_every=1000\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate adaptive hybrid with checkpointing + optional subsampling.\n",
    "    - user_to_idx_itemuser: visitorid -> user col idx in item_user_sparse\n",
    "    - max_eval_users: if >0, sample up to that many users for evaluation\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Adaptive hybrid eval (memory-safe). Preparing users...\")\n",
    "    true_items_per_user = test_df.groupby(\"visitorid\")[\"itemid\"].apply(list).to_dict()\n",
    "    all_users = list(true_items_per_user.keys())\n",
    "    total_users = len(all_users)\n",
    "    if max_eval_users and total_users > max_eval_users:\n",
    "        rng = np.random.default_rng(42)\n",
    "        sampled_users = list(rng.choice(all_users, size=max_eval_users, replace=False))\n",
    "        print(f\"‚ö†Ô∏è Will evaluate on subset {len(sampled_users):,}/{total_users:,} users (max_eval_users={max_eval_users})\")\n",
    "    else:\n",
    "        sampled_users = all_users\n",
    "\n",
    "    cache_path = os.path.join(CACHE_DIR, \"hybrid_adaptive_eval_checkpoint.pkl\")\n",
    "    results_list = []\n",
    "    start_idx = 0\n",
    "\n",
    "    # resume if checkpoint exists\n",
    "    if os.path.exists(cache_path):\n",
    "        try:\n",
    "            d = load_pickle(cache_path)\n",
    "            results_list = d.get(\"results\", [])\n",
    "            start_idx = d.get(\"index\", 0)\n",
    "            print(f\"Resuming evaluation from checkpoint at user index {start_idx}\")\n",
    "        except Exception:\n",
    "            print(\"Failed to load checkpoint, starting fresh.\")\n",
    "\n",
    "    precisions, recalls, hitrates, ndcgs = [], [], [], []\n",
    "    all_recommended = set()\n",
    "\n",
    "    for i in range(start_idx, len(sampled_users)):\n",
    "        uid = sampled_users[i]\n",
    "        true_items = true_items_per_user[uid]\n",
    "        recs = hybrid_weighted_recommend_adaptive(\n",
    "            uid, train_df,\n",
    "            item_user_sparse, idx_to_itemid_itemuser,\n",
    "            item_neighbors_idx, item_neighbors_sims,\n",
    "            itemuser_idx_to_emb_idx, idx_to_itemid_emb_arr, emb_index, emb,\n",
    "            user_to_idx_itemuser,\n",
    "            top_n=k, min_alpha=min_alpha, max_alpha=max_alpha\n",
    "        )\n",
    "        if not recs:\n",
    "            # still want progress; skip\n",
    "            if i % 1000 == 0:\n",
    "                print(f\"Processed {i}/{len(sampled_users)} users (no recs for uid={uid})\")\n",
    "            continue\n",
    "\n",
    "        p, r = precision_recall_at_k(recs, true_items, k=k)\n",
    "        ndcg = ndcg_at_k(recs, true_items, k=k)\n",
    "        hit = 1.0 if any(it in true_items for it in recs) else 0.0\n",
    "\n",
    "        precisions.append(p); recalls.append(r); hitrates.append(hit); ndcgs.append(ndcg)\n",
    "        all_recommended.update(recs)\n",
    "\n",
    "        # checkpoint\n",
    "        if (i + 1) % checkpoint_every == 0:\n",
    "            save_pickle({\"results\": results_list, \"index\": i + 1}, cache_path)\n",
    "            print(f\"üíæ Checkpoint saved at user {i + 1}/{len(sampled_users)}\")\n",
    "\n",
    "    # final aggregate\n",
    "    results = {\n",
    "        \"Precision@5\": float(np.mean(precisions)) if precisions else 0.0,\n",
    "        \"Recall@5\": float(np.mean(recalls)) if recalls else 0.0,\n",
    "        \"HitRate@5\": float(np.mean(hitrates)) if hitrates else 0.0,\n",
    "        \"Coverage\": float(len(all_recommended) / max(1, len(idx_to_itemid_itemuser))),\n",
    "        \"NDCG@5\": float(np.mean(ndcgs)) if ndcgs else 0.0\n",
    "    }\n",
    "\n",
    "    # cleanup checkpoint\n",
    "    if os.path.exists(cache_path):\n",
    "        try:\n",
    "            os.remove(cache_path)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return pd.DataFrame([results])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2917824e",
   "metadata": {},
   "source": [
    "# **Main Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6637e41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"Loading data...\")\n",
    "    events, item_props, category_tree = load_data()\n",
    "    events = preprocess_events(events)\n",
    "\n",
    "    print(\"Train/Test Split...\")\n",
    "    train_df, test_df = train_test_split_by_last_n(events, TRAIN_TEST_LAST_N)\n",
    "\n",
    "    print(\"Building features...\")\n",
    "    item_features = build_item_category_matrix(item_props, category_tree)\n",
    "    emb, itemid_to_emb_idx_map, idx_to_itemid_emb_arr = build_cb_embeddings(item_features)\n",
    "    emb_index = build_hnsw_index(emb)\n",
    "\n",
    "    print(\"Building CF...\")\n",
    "    item_user_sparse, idx_to_itemid_itemuser, item_to_idx_itemuser, user_to_idx_itemuser = build_item_user_matrix(train_df)\n",
    "    item_neighbors_idx, item_neighbors_sims = build_item_neighbors(item_user_sparse, ITEM_NN_TOPK)\n",
    "\n",
    "    # ‚úÖ Sinkronisasi CF dan CB\n",
    "    valid_items_cf = set(idx_to_itemid_itemuser)\n",
    "    valid_items_cb = set(itemid_to_emb_idx_map.keys())\n",
    "    common_items = list(valid_items_cf & valid_items_cb)\n",
    "    print(f\"Common items CF ‚à© CB: {len(common_items):,}\")\n",
    "\n",
    "    item_user_sparse = item_user_sparse[[i for i, it in enumerate(idx_to_itemid_itemuser) if it in common_items], :]\n",
    "    idx_to_itemid_itemuser = [it for it in idx_to_itemid_itemuser if it in common_items]\n",
    "\n",
    "    # ‚úÖ Mapping CF index ‚Üí CB embedding index\n",
    "    itemuser_idx_to_emb_idx = {}\n",
    "    for it_idx, itemid in enumerate(idx_to_itemid_itemuser):\n",
    "        if itemid in itemid_to_emb_idx_map:\n",
    "            itemuser_idx_to_emb_idx[it_idx] = itemid_to_emb_idx_map[itemid]\n",
    "\n",
    "    print(\"Evaluating adaptive hybrid recommender...\")\n",
    "    results_df = evaluate_hybrid_item_cf_adaptive(\n",
    "        train_df, test_df, item_features,\n",
    "        emb, itemuser_idx_to_emb_idx, idx_to_itemid_emb_arr, emb_index,\n",
    "        item_user_sparse, idx_to_itemid_itemuser,\n",
    "        item_neighbors_idx, item_neighbors_sims,\n",
    "        user_to_idx_itemuser,\n",
    "        min_alpha=0.1, max_alpha=0.9, k=K_EVAL,\n",
    "        max_eval_users=50000,\n",
    "        checkpoint_every=5000\n",
    "    )\n",
    "\n",
    "    print(\"\\n===== Final Adaptive Evaluation Results =====\")\n",
    "    print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "74b8c3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Train/Test Split...\n",
      "Building features...\n",
      "Building CF...\n",
      "Common items CF ‚à© CB: 180,664\n",
      "Evaluating adaptive hybrid recommender...\n",
      "Adaptive hybrid eval (memory-safe). Preparing users...\n",
      "‚ö†Ô∏è Will evaluate on subset 50,000/120,416 users (max_eval_users=50000)\n",
      "Processed 2000/50000 users (no recs for uid=1062578)\n",
      "üíæ Checkpoint saved at user 5000/50000\n",
      "üíæ Checkpoint saved at user 10000/50000\n",
      "Processed 10000/50000 users (no recs for uid=1349837)\n",
      "Processed 11000/50000 users (no recs for uid=1399404)\n",
      "üíæ Checkpoint saved at user 15000/50000\n",
      "üíæ Checkpoint saved at user 20000/50000\n",
      "üíæ Checkpoint saved at user 25000/50000\n",
      "üíæ Checkpoint saved at user 30000/50000\n",
      "Processed 31000/50000 users (no recs for uid=250712)\n",
      "üíæ Checkpoint saved at user 35000/50000\n",
      "üíæ Checkpoint saved at user 40000/50000\n",
      "üíæ Checkpoint saved at user 45000/50000\n",
      "Processed 45000/50000 users (no recs for uid=669949)\n",
      "üíæ Checkpoint saved at user 50000/50000\n",
      "\n",
      "===== Final Adaptive Evaluation Results =====\n",
      " Precision@5  Recall@5  HitRate@5  Coverage   NDCG@5\n",
      "    0.001866  0.003832   0.009045  0.252391 0.002739\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
